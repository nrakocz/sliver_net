
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/spatiailMaxPooling.ipynb

from exp.nb_databunch import *
from exp.fastai_imports import *
from exp.nb_AmishDataLoaders import *
from exp.amish_sites import *
from exp.nb_databunch import *


class SpatialMaxPooling(nn.Module):

    def __init__(self,f_map_size,steps,w_avg=None):
        super(SpatialMaxPooling,self).__init__()
        self.f_map_size = f_map_size
        self.steps = steps
        self.w_avg = w_avg

    def forward(self,x):

        #get max
        _,ind_tensor = F.adaptive_max_pool2d_with_indices (x,(1,1),return_indices=True)

        #find indices spatialy close to max
        padding = self.f_map_size*self.steps
        spatial_max_ind = get_spatial_max_ind(ind_tensor,padding,self.f_map_size,self.steps)
        if(x.is_cuda):
            spatial_max_ind = spatial_max_ind.cuda()

        #flatten
        x_flat = x.view(x.size(0), -1) if x.dim()<4 else x.view(x.size(0),x.size(1), -1)

        #pad
        x_pad = F.pad(x_flat,(padding,padding))

        # gather
        y = x_pad.gather(-1,spatial_max_ind)

        #weighted sum
        if(self.w_avg is not None):
            return y@w_avg.view(-1,1)

        return y.mean(dim=-1)


def get_spatial_max_ind(ind_tensor,padding,F_IMG_SIZE,STEPS):

    # NO BATCHES
    if(ind_tensor.dim()<4):
        return torch.stack(channelItter(batch,padding,F_IMG_SIZE,STEPS))

    # WITH BATCHES
    batches_maxs = []
    for batch in ind_tensor:
        batches_maxs.append(torch.stack(channelItter(batch,padding,F_IMG_SIZE,STEPS)))

    return torch.stack(batches_maxs).squeeze()



def channelItter(ind_tensor,padding,F_IMG_SIZE,STEPS):
    l = []
    for fmap in ind_tensor.squeeze(2):
        ind = fmap[0] + padding
        fmap_new_ind = torch.tensor([ind+i*F_IMG_SIZE for i in range(-STEPS,STEPS+1)])
        l.append(fmap_new_ind)

    return l